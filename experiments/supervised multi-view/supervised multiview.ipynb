{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graspy\n",
    "from graphstats import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses a simple probability equality to validate leveraging node attributes to potentially decrease misclassification rates in a simple network setting. That is, consider the setting where \n",
    "$$ ((X_{i}, Z_{i}), Y_{i}) \\sim_{iid} F_{X,Z,Y} \\; i = 1, ..., m $$ In the RDPG setting, we can consider $ X_{i} $ as the latent position for node $ i $, $ Z_{i} \\in \\mathbb{R}^{p} $ is a 'node covariate' and $ Y_{i} \\in [K] $ the class label of node $ i $. Assume that $ X_{i}, Z_{i} $ are independent conditioned on $ Y_{i} $, then\n",
    "\\begin{align*}\n",
    "P(X, Y, Z) &= P(X, Z|Y)P(Y) \\\\\n",
    "&= P(X|Y)P(Z|Y)P(Y) \\\\\n",
    "&= P(X|Y)P(Y|Z)P(Z) \\\\\n",
    "\\end{align*} The second and third lines are both easily interpretable -- the second line is simply the product of the conditional likelihoods scaled by the likelihood of oberving a particular label and the third line the product of the conditional likelihood of a particular latent position and the posterior of a label given only the node covariate scaled by the likelihood of the node covariate. \n",
    "\n",
    "For a particular unlabeled node, we observe $ X \\in \\mathcal{X} $ and $ Z \\in \\mathcal{Z} $. Conditioning on $ X = x, Z = z $, we consider the two quantities\n",
    "$$ P(X=x|Y)P(Z=z|Y)P(Y) \\& P(X=x|Y)P(Y|Z=z)P(Z=z) $$ where we can maximize over possible values of $ y \\in [K] $. Any classifier that attempts to maximize the first (corresponding the second line above) requires conditional density estimates of $ x $ and $ z $. In the RDPG/SBM setting, we can safely assume that the density of $ X|Y $ is Gaussian. Unless we make assumptions on the density of $ Z|Y $ we must use nonparametric density estimation. For $ n $ small this may not be tenable. $ P(Y) $ is efficiently estimable via $ \\frac{n_{k}}{m} $.\n",
    "\n",
    "On the other hand we may consider a classifier implied by the second product. The conditional density of X given Y is still approximately Gaussian. The term P(Y|Z=z) is simply the posterior density of $ Y $ conditioned on the particular $ z $. In the machine learning literature, there exists a plethora of methods to estimate this -- random forests, k Nearest Neighbors, etc. For the purposes of maximizing the product over elements of $ [K] $ P(Z=z) is unimportant and hence can be ignored. Or simply\n",
    "$$ \\argmax_{u \\in [K]} P(X=x|Y=u)P(Y=u|Z=z)P(Z=z) = \\argmax_{u \\in [K]} P(X=x|Y=u)P(Y=u|Z=z) $$\n",
    "\n",
    "Note that both of the two products take advantage of the conditional independence of $ X, Z $ and can thus naturally leverage the fact that the conditional density of $ X $ is asymptotically Gaussian. This is better than, say, using a random forest for both $ X $ and $ Z $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
