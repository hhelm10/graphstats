{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status - As of 4/19/2019 QDA > HHRF > HHKNN > RF > KNN\n",
    "# Having trouble getting RF to perform well.\n",
    "\n",
    "from covariates_gclass import *\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "sns.set()\n",
    "import _pickle as pickle\n",
    "truth = pickle.load(open('LL1_truth.pkl', 'rb'))\n",
    "train = pickle.load(open('LL1_train.pkl', 'rb'))\n",
    "test = pickle.load(open('LL1_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, Z, normal_params, fitted_model, m = None):\n",
    "    \"\"\"\n",
    "    Classifies vertices.\n",
    "\n",
    "    X - n x p; Normally distributed random variables.\n",
    "    Z - n x d; Not normally distributed random variables.\n",
    "    fitted_model - A sklearn model that can return posterior estimates.\n",
    "    m - number of training data used to train fitted_model.\n",
    "    \"\"\"\n",
    "\n",
    "    n, p = X.shape\n",
    "    m, d = Z.shape\n",
    "    \n",
    "    K = len(normal_params)\n",
    "    \n",
    "    if n != m:\n",
    "        raise ValueError('different number of samples for X, Z')\n",
    "    \n",
    "    if p == 1:\n",
    "        norm_pdf = norm.pdf\n",
    "        X = X.reshape((1, -1))[0]\n",
    "    else:\n",
    "        norm_pdf = mvn.pdf\n",
    "        \n",
    "    posteriors = fitted_model.predict_proba(Z)\n",
    "    \n",
    "    predictions=-1*np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        print(posteriors[i])\n",
    "        if m is None:\n",
    "            smoothed_posterior = posteriors[i]\n",
    "        else:\n",
    "            posterior_plus = posteriors[i] + np.ones(K)/m\n",
    "            smoothed_posterior = posterior_plus / np.sum(posterior_plus)\n",
    "        temp_pdfs = np.array([norm_pdf(X[i], normal_params[j][0], normal_params[j][1]) for j in range(K)])\n",
    "        posterior_pdf_prod = temp_pdfs * smoothed_posterior\n",
    "        predictions[i] = int(np.argmax(posterior_pdf_prod))\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = truth['learningData']['classLabel'].values.astype(int)\n",
    "G = train['0']\n",
    "n = len(G)\n",
    "\n",
    "A = nx.to_numpy_array(G)\n",
    "train_labels = train['learningData']['classLabel'].values.astype(int)\n",
    "training_idx = train['learningData']['d3mIndex'].values.astype(int)\n",
    "\n",
    "unique_labels, n_seeds = np.unique(train_labels, return_counts=True)\n",
    "K = len(unique_labels)\n",
    "class_train_idx = [np.where(train_labels == i)[0] for i in unique_labels]\n",
    "\n",
    "train_idx = np.concatenate((class_train_idx)).astype(int)\n",
    "test_idx = [k for k in range(n) if k not in train_idx]\n",
    "labels = true_labels[true_labels[test_idx]]\n",
    "\n",
    "MORE_ATTR = True\n",
    "attr_number = 1\n",
    "attrs = []\n",
    "while MORE_ATTR:\n",
    "    attr = 'attr'\n",
    "    temp_attr = list(nx.get_node_attributes(G, 'attr' + str(attr_number)).values())\n",
    "    if len(temp_attr) == 0:\n",
    "        MORE_ATTR = False\n",
    "    else:\n",
    "        attrs.append(temp_attr)\n",
    "        attr_number += 1\n",
    "attrs = np.array(attrs).T\n",
    "for i in range(attrs.shape[1]):\n",
    "    attrs[:, i] = attrs[:, i]/max(attrs[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20192491 0.54788931 0.25018579]\n",
      "[0.03785714 0.23812404 0.72401881]\n",
      "[0.12462762 0.25061783 0.62475455]\n",
      "[0.25798477 0.60726068 0.13475455]\n",
      "[0.29382831 0.49088231 0.21528938]\n",
      "[0.03833333 0.18337279 0.77829387]\n",
      "[0.03833333 0.19337279 0.76829387]\n",
      "[0.05457207 0.15611508 0.78931285]\n",
      "[0.21368407 0.59896379 0.18735215]\n",
      "[0.05607143 0.26473119 0.67919739]\n",
      "[0.67389835 0.27069689 0.05540476]\n",
      "[0.18389835 0.63690426 0.17919739]\n",
      "[0.03785714 0.22812404 0.73401881]\n",
      "[0.15514835 0.60565426 0.23919739]\n",
      "[0.24389835 0.62240426 0.13369739]\n",
      "[0.12272677 0.34708744 0.53018579]\n",
      "[0.0990252  0.15568543 0.74528938]\n",
      "[0.28825733 0.58407093 0.12767174]\n",
      "[0.25531777 0.56449645 0.18018579]\n",
      "[0.03212762 0.22113889 0.74673349]\n",
      "[0.03833333 0.19337279 0.76829387]\n",
      "[0.68416026 0.21019689 0.10564286]\n",
      "[0.18368407 0.60229712 0.21401881]\n",
      "[0.0990252  0.15568543 0.74528938]\n",
      "[0.20472835 0.6171044  0.17816725]\n",
      "[0.05607143 0.26473119 0.67919739]\n",
      "[0.2611511  0.57866311 0.16018579]\n",
      "[0.28531777 0.56116311 0.15351912]\n",
      "[0.03833333 0.18337279 0.77829387]\n",
      "[0.08851465 0.22696623 0.68451912]\n",
      "[0.67368407 0.24233974 0.08397619]\n",
      "[0.68416026 0.21019689 0.10564286]\n",
      "[0.29798477 0.58503846 0.11697677]\n",
      "[0.10722835 0.66653816 0.22623349]\n",
      "[0.03833333 0.18337279 0.77829387]\n",
      "[0.13272677 0.33708744 0.53018579]\n",
      "[0.03833333 0.18337279 0.77829387]\n",
      "[0.18389835 0.63690426 0.17919739]\n",
      "[0.18368407 0.60229712 0.21401881]\n",
      "[0.04607143 0.26473119 0.68919739]\n",
      "[0.20389835 0.63357093 0.16253072]\n",
      "[0.20416026 0.55754587 0.23829387]\n",
      "[0.0990252  0.15568543 0.74528938]\n",
      "[0.66416026 0.22519689 0.11064286]\n",
      "[0.67798477 0.27665626 0.04535897]\n",
      "[0.11521676 0.14780556 0.73697769]\n",
      "[0.11521676 0.14780556 0.73697769]\n",
      "[0.12462762 0.25061783 0.62475455]\n",
      "[0.21130311 0.55040301 0.23829387]\n",
      "[0.03785714 0.22812404 0.73401881]\n",
      "[0.21368407 0.59896379 0.18735215]\n",
      "[0.77382831 0.14708367 0.07908802]\n",
      "[0.29382831 0.49088231 0.21528938]\n",
      "[0.03785714 0.22812404 0.73401881]\n",
      "[0.12462762 0.25061783 0.62475455]\n",
      "[0.09184799 0.22796623 0.68018579]\n",
      "[0.11521676 0.14780556 0.73697769]\n",
      "[0.74781777 0.20523352 0.04694872]\n",
      "[0.05607143 0.25473119 0.68919739]\n",
      "[0.02546096 0.23859402 0.73594503]\n",
      "[0.26056566 0.53589927 0.20353508]\n",
      "[0.66056566 0.22382284 0.1156115 ]\n",
      "[0.05607143 0.26473119 0.67919739]\n",
      "[0.16389502 0.68449328 0.15161169]\n",
      "[0.22906777 0.56074645 0.21018579]\n",
      "[0.28531777 0.56116311 0.15351912]\n",
      "[0.03212762 0.22256746 0.74530492]\n",
      "[0.21389835 0.63357093 0.15253072]\n",
      "[0.16489502 0.63837149 0.19673349]\n",
      "[0.67389835 0.27069689 0.05540476]\n",
      "[0.09184799 0.21796623 0.69018579]\n",
      "[0.23757831 0.48713231 0.27528938]\n",
      "[0.03833333 0.18337279 0.77829387]\n",
      "[0.12462762 0.25061783 0.62475455]\n",
      "[0.09184799 0.22796623 0.68018579]\n",
      "[0.75781777 0.19386988 0.04831235]\n",
      "[0.02546096 0.25830556 0.71623349]\n",
      "[0.0990252  0.15568543 0.74528938]\n",
      "[0.25798477 0.60726068 0.13475455]\n",
      "[0.03833333 0.18337279 0.77829387]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0. 0. 1.]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0. 0. 1.]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0. 0. 1.]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0. 1. 0.]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0. 1. 0.]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0. 0. 1.]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0.66666667 0.33333333 0.        ]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0.66666667 0.33333333 0.        ]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0. 1. 0.]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0. 0. 1.]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0.33333333 0.33333333 0.33333333]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 0. 1.]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[1. 0. 0.]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n",
      "[0.66666667 0.33333333 0.        ]\n",
      "[0. 0. 1.]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0. 0. 1.]\n",
      "[0.         0.66666667 0.33333333]\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0.         0.33333333 0.66666667]\n",
      "[0.33333333 0.66666667 0.        ]\n",
      "[0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.5375],\n",
       " [0.42500000000000004],\n",
       " [0.38749999999999996],\n",
       " [0.3125],\n",
       " [0.17500000000000004]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- Total number of seeds\n",
    "m = np.sum(n_seeds)\n",
    "\n",
    "#- estimate class probabilities\n",
    "pi_hats = n_seeds / m\n",
    "\n",
    "ase_obj = ASE(n_elbows=1)\n",
    "X = ase_obj.fit_transform(A)\n",
    "\n",
    "Z = attrs\n",
    "\n",
    "XZ = np.concatenate((X, Z), axis=1)\n",
    "\n",
    "#- Store mvn samples corresponding to seeds\n",
    "seeds_norm = X[train_idx]\n",
    "\n",
    "#- Estimate normal parameters using seeds\n",
    "mu1, cov1 = estimate_normal_parameters(X[class_train_idx[0]])\n",
    "params1 = [mu1, cov1]\n",
    "\n",
    "mu2, cov2 = estimate_normal_parameters(X[class_train_idx[1]])\n",
    "params2 = [mu2, cov2]\n",
    "\n",
    "mu3, cov3 = estimate_normal_parameters(X[class_train_idx[2]])\n",
    "params3 = [mu3, cov3]\n",
    "\n",
    "#- Convenient way to store\n",
    "params=[params1, params2, params3]\n",
    "\n",
    "#- Store uniform samples corresponding to seeds\n",
    "seeds_beta = Z[train_idx]\n",
    "\n",
    "#- Using conditional indendence assumption (RF, KNN used for posterior estimates)\n",
    "# if errors is None:\n",
    "errors = [[] for i in range(5)]\n",
    "\n",
    "temp_pred = QDA(X[test_idx], pi_hats, params)\n",
    "temp_error = 1 - np.sum(temp_pred == true_labels[test_idx])/len(test_idx)\n",
    "errors[0].append(temp_error)\n",
    "\n",
    "rf1 = RF(n_estimators=100, max_depth=int(np.round(np.log(seeds_beta.shape[0]/2))))\n",
    "rf1.fit(seeds_beta, true_labels[train_idx])\n",
    "\n",
    "knn1 = KNN(n_neighbors=int(np.round(np.log(seeds_beta.shape[0]))))\n",
    "knn1.fit(seeds_beta, true_labels[train_idx])\n",
    "\n",
    "smooth = True\n",
    "\n",
    "if smooth:\n",
    "    temp_pred = classify(X[test_idx], Z[test_idx], params, rf1, m = m)\n",
    "    temp_error = 1 - np.sum(temp_pred == true_labels[test_idx])/len(test_idx)\n",
    "    errors[1].append(temp_error)\n",
    "\n",
    "    temp_pred = classify(X[test_idx], Z[test_idx], params, knn1, m = m)\n",
    "    temp_error = 1 - np.sum(temp_pred == true_labels[test_idx])/len(test_idx)\n",
    "    errors[2].append(temp_error)\n",
    "else:\n",
    "    temp_pred = classify(X[test_idx], Z[test_idx], params, rf1)\n",
    "    temp_error = 1 - np.sum(temp_pred == true_labels[test_idx])/len(test_idx)\n",
    "    errors[1].append(temp_error)\n",
    "\n",
    "    temp_pred = classify(X[test_idx], Z[test_idx], params, knn1)\n",
    "    temp_error = 1 - np.sum(temp_pred == true_labels[test_idx])/len(test_idx)\n",
    "    errors[2].append(temp_error)\n",
    "\n",
    "#- Not using conditional independence assumption (RF, KNN used for classification)\n",
    "XZseeds = np.concatenate((seeds_norm, seeds_beta), axis=1)\n",
    "\n",
    "rf2 = RF(n_estimators=10, max_depth=int(np.round(np.log(m))))\n",
    "rf2.fit(XZseeds, true_labels[train_idx])\n",
    "temp_pred = rf2.predict(XZ[test_idx])\n",
    "temp_error = 1 - np.sum(temp_pred == true_labels[test_idx])/len(test_idx)\n",
    "errors[3].append(temp_error)\n",
    "\n",
    "knn2 = KNN(n_neighbors=int(np.round(np.log(m))))\n",
    "knn2.fit(XZseeds, true_labels[train_idx])\n",
    "\n",
    "temp_pred = knn2.predict(XZ[test_idx])\n",
    "temp_error = 1 - np.sum(temp_pred == true_labels[test_idx])/len(test_idx)\n",
    "errors[4].append(temp_error)\n",
    "\n",
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh",
   "language": "python",
   "name": "hh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
